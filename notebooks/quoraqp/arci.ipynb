{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Arc-I Model\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NTMC-Community/MatchZoo/2.0/artworks/matchzoo-logo.png\" alt=\"logo\" style=\"width:600px;float: center\"/>\n",
    "\n",
    "This is a tutorial on training *Arc-I Model* [Hu et al. 2014](http://papers.nips.cc/paper/5550-convolutional-neural-network-architectures-for-matching-natural-language-sentences.pdf) model with [MatchZoo](https://github.com/faneshion/MatchZoo)for **classification task**. We use [QuoraQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) as the example benchmark data set to show the usage.\n",
    "\n",
    "*To walk through this notebook, you need approx 90 minutes.*\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/pangliang/nips/venv/lib/python36.zip', '/home/pangliang/nips/venv/lib/python3.6', '/home/pangliang/nips/venv/lib/python3.6/lib-dynload', '/usr/local/python3/lib/python3.6', '/home/pangliang/nips/venv/lib/python3.6/site-packages', '/home/pangliang/nips/playground_pl', '/home/pangliang/matching/MatchZoo_New', '/home/pangliang/nips/venv/lib/python3.6/site-packages/IPython/extensions', '/home/pangliang/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TL;DR**\n",
    "\n",
    "The following code block illustrates the main workflow of how to train a Arc-I model. \n",
    "\n",
    "```python\n",
    "from matchzoo import preprocessor\n",
    "from matchzoo import generators\n",
    "from matchzoo import models\n",
    "\n",
    "train, test = ... # prepare your training data and test data.\n",
    "\n",
    "arci_preprocessor = preprocessor.ArcIPreprocessor()\n",
    "processed_tr = arci_preprocessor.fit_transform(train, stage='train')\n",
    "processed_te = arci_preprocessor.fit_transform(test, stage='predict')\n",
    "\n",
    "\n",
    "generator_tr = generators.PointGenerator(processed_tr)\n",
    "generator_te = generators.PointGenerator(processed_te)\n",
    "# Example, train with generator, test with the first batch.\n",
    "X_te, y_te = generator_te[0]\n",
    "\n",
    "arci_model = models.ArcIModel()\n",
    "arci_model.guess_and_fill_missing_params()\n",
    "arci_model.build()\n",
    "arci_model.compile()\n",
    "arci_model.fit_generator(generator_tr)\n",
    "# Make predictions\n",
    "predictions = arci_model.predict([X_te.text_left, X_te.text_right])\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MatchZoo expect a list of *Quintuple* as training data:\n",
    "\n",
    "```python\n",
    "train = [('qid0', 'did0', 'query 0', 'document 0', 'label 0'),\n",
    "         ('qid0', 'did1', 'query 0', 'document 1', 'label 1'),\n",
    "          ...,\n",
    "         ('qid1', 'did2', 'query 1', 'document 2', 'label 3')]\n",
    "```\n",
    "\n",
    "The corresponded columns are `(text_left_id, text_right_id, text_left, text_right, label)`. For Information Retrieval task, *text_left* is referred as *query*, and *text_right* is document.\n",
    "\n",
    "For the test case, MatchZoo expect a list of *Quadruple* (we do not need labels) as input:\n",
    "\n",
    "```python\n",
    "test = [('qid9', 'did5', 'query 9', 'document 5'),\n",
    "         ...,\n",
    "        ('qid2', 'did7', 'query 2', 'document 7')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Content\n",
    "\n",
    "+ Prepare QuoraQP dataset\n",
    "    - Download\n",
    "    - Load\n",
    "    - Adjustment\n",
    "+ Preprocessing\n",
    "+ Data Generator\n",
    "+ Model Training\n",
    "    - Initialize\n",
    "    - Hyper-Parameters\n",
    "    - Make Prediction\n",
    "    - Model Persistence\n",
    "- Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Quora Question Pair dataset\n",
    "\n",
    "#### Download\n",
    "\n",
    "We take QuoraQP as the example benchmark dataset to show the usage of MatchZoo. Firstly you need to downlowd the data and uncompress the data, currently you need to download the dataset via [kaggle](https://www.kaggle.com/c/quora-question-pairs/data). Unzip the data, you'll get\n",
    "\n",
    "- train.csv\n",
    "- test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Adjustment\n",
    "\n",
    "The *train/test* files of QuoraQP are *train.csv*,  *test.csv* under the uncompressed folder QuoraQP. \n",
    "\n",
    "We can convert this format to the expected input format of MatchZoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  id           qid1           qid2   is_duplicate\n",
      "count  404290.000000  404290.000000  404290.000000  404290.000000\n",
      "mean   202144.500000  217243.942418  220955.655337       0.369198\n",
      "std    116708.614503  157751.700002  159903.182629       0.482588\n",
      "min         0.000000       1.000000       2.000000       0.000000\n",
      "25%    101072.250000   74437.500000   74727.000000       0.000000\n",
      "50%    202144.500000  192182.000000  197052.000000       0.000000\n",
      "75%    303216.750000  346573.500000  354692.500000       1.000000\n",
      "max    404289.000000  537932.000000  537933.000000       1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pangliang/nips/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2901: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        test_id question1 question2\n",
      "count   3563475   3563475   3563475\n",
      "unique  2607940   2211009   2227400\n",
      "top     1303969     What      What \n",
      "freq          2      2033      2016\n"
     ]
    }
   ],
   "source": [
    "data_folder = '../../data/quoraqp/'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_data(file_path, stage):\n",
    "    df = pd.read_csv(file_path, dtype={'question1': 'str', 'question2': 'str'}, keep_default_na=False)\n",
    "    print(df.describe())\n",
    "    if stage == 'train':\n",
    "        df = df[['qid1', 'qid2', 'question1', 'question2', 'is_duplicate']]\n",
    "    elif stage == 'predict':\n",
    "        df = df[['question1', 'question2']]\n",
    "        # assign ids to left and right\n",
    "        q_a = pd.unique(df.values.ravel())\n",
    "        # add index for each\n",
    "        q_a = dict(map(lambda t: (t[1], t[0]), enumerate(q_a)))\n",
    "        # assign id\n",
    "        df['qid1'] = df['question1'].map(q_a)\n",
    "        df['qid2'] = df['question2'].map(q_a)\n",
    "        # change the order of columns\n",
    "        cols = ['qid1', 'qid2', 'question1', 'question2']\n",
    "        df = df[cols]\n",
    "    # convert dataframe into list of tuples\n",
    "    qa_pairs = [tuple(x) for x in df.values]\n",
    "    return qa_pairs\n",
    "    \n",
    "\n",
    "train = read_data(data_folder + 'train.csv', stage='train')\n",
    "test = read_data(data_folder + 'test.csv', stage='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download\n",
    "\n",
    "We first download the GLoVe pre-trained word embeddings. Be pationt - this takes a long time related to your network service. Alternatively, you can download it via another downloading tool and put **glove.840B.300d.txt** in the directory **../../data/embedding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-20 20:20:26--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
      "正在解析主机 nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "正在连接 nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 302 Found\n",
      "位置：https://nlp.stanford.edu/data/glove.840B.300d.zip [跟随至新的 URL]\n",
      "--2018-10-20 20:20:27--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
      "正在连接 nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：2176768927 (2.0G) [application/zip]\n",
      "正在保存至: “../../data/embedding/glove.840B.300d.zip.5”\n",
      "\n",
      "glove.840B.300d.zip 100%[===================>]   2.03G   535KB/s  用时 52m 54s   \n",
      "\n",
      "2018-10-20 21:13:22 (670 KB/s) - 已保存 “../../data/embedding/glove.840B.300d.zip.5” [2176768927/2176768927])\n",
      "\n",
      "Archive:  ../../data/embedding/glove.840B.300d.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of ../../data/embedding/glove.840B.300d.zip or\n",
      "        ../../data/embedding/glove.840B.300d.zip.zip, and cannot find ../../data/embedding/glove.840B.300d.zip.ZIP, period.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ../../data/embedding\n",
    "!wget -O ../../data/embedding/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip \n",
    "!unzip -o ../../data/embedding/glove.840B.300d.zip -d ../../data/embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preproceesing with pre-trained embeddings\n",
    "\n",
    "We run the processor in three lines of code.  \n",
    "If we use **the randomly initialized embeddings**, just keep default value for *embedding_file*, and the preprocessing code is as follows:\n",
    "```python\n",
    "from matchzoo import preprocessor\n",
    "arci_preprocessor = preprocessor.ArcIPreprocessor()\n",
    "processed_tr = arci_preprocessor.fit_transform(train, stage='train')\n",
    "processed_te = arci_preprocessor.fit_transform(test, stage='predict')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dssm preprocessor.\n",
    "from matchzoo import preprocessor\n",
    "arci_preprocessor = preprocessor.ArcIPreprocessor(embedding_file='../../data/embedding/glove.840B.300d.txt')\n",
    "processed_tr = arci_preprocessor.fit_transform(train, stage='train')\n",
    "processed_te = arci_preprocessor.fit_transform(test, stage='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `processed_tr`?**\n",
    "\n",
    "`processed_tr` is a **MatchZoo DataPack** data structure (see `matchzoo/datapack.py`). It contains \n",
    "1. A *2-columns* `pandas DataFrame` called `left` to host all the pre-processed records including index and processed text to store `text_left` and `id_left`.\n",
    "2. A *2-columns* `pandas DataFrame` called `right` to host all the pre-processed records including index and processed text to store `text_right` and `id_right`.\n",
    "3. A *2-columns* `pandas DataFrame` called `relation` to host all the pre-processed records including index and index mapping `id_left` and `id_right`.\n",
    "4. A `context` property (dictionary) consists of all the parameters fitted during pre-processing. \n",
    "\n",
    "\n",
    "The `fit_transform` method is a sequential combination of two methods:\n",
    "\n",
    "1. Fit parameters using the `fit` function, this only happens when `stage='train'`.\n",
    "2. Transform data into expected format.\n",
    "\n",
    "So the previous preprocessing code in test stage can also be written as:\n",
    "\n",
    "```python\n",
    "processed_te = dssm_preprocessor.transform(test, stage='predict')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described, the fitted parameters were stored in `context` property, to access the context, just call:\n",
    "\n",
    "```python\n",
    "print(processed_tr.context)\n",
    "```\n",
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  3707\n"
     ]
    }
   ],
   "source": [
    "print('vocab size: ', len(processed_tr.context['term_index']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What has been stored in the `context?`** \n",
    "\n",
    "We stored `term_index` and `input_shapes` in the context property. \n",
    "\n",
    "\n",
    "**What is `arci_preprocessor` actually doing?**\n",
    "\n",
    "The `arci_preprocessor` is calling a sequence of `process_units`. Each `process_unit` is designed to perform one atom operation on input data. For instance, in `arci_preprocessor`, we called:\n",
    "\n",
    "\n",
    "1. TokenizeUnit: Perform tokenization on raw input data.\n",
    "2. LowercaseUnit: Transform all tokens into lower case.\n",
    "3. PuncRemovalUnit: Remove all the punctuations.\n",
    "4. StopRemovalUnit: Remove all the stopwords.\n",
    "\n",
    "Depending on whether using the pre-trained embeddings, the preprocessor loads the embedding from an embedding file or randomly initializes the embeddings.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### Data Generation\n",
    "\n",
    "For memory efficiency, we expect you to use **generator** to generate batches of data on the fly. For example, we can create a **PointGenerator** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchzoo import generators\n",
    "from matchzoo import tasks\n",
    "generator_tr = generators.PointGenerator(inputs=processed_tr, task=tasks.Classification(), batch_size=64, stage='train')\n",
    "generator_te = generators.PointGenerator(inputs=processed_te, task=tasks.Classification(), batch_size=64, stage='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first batch of trainig data, just call `X_train, y_train = generator[0]`.\n",
    "\n",
    "**What is PointGenerator?**\n",
    "**PointGenerator** is this case, it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then the problem can be approximated by a regression/Classification problem — given a single query-document pair, predict its score.\n",
    "\n",
    "A number of existing supervised machine learning algorithms can be readily used for this purpose. Ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.\n",
    "\n",
    "**What is PairGenerator?**\n",
    "In this case, the problem is approximated by a classification problem — learning a binary classifier that can tell which document is better in a given pair of documents.\n",
    "\n",
    "In MatchZoo, **PairGenerator** generate one positive & `num_neg` negative examples per pair. As an example, to train an Arc-I model (for document ranking), we use `num_neg=4`. \n",
    "\n",
    "**What is ListGenerator?**\n",
    "This generator try to directly optimize the value of evaluation measures, averaged over all queries in the training data. \n",
    "\n",
    "Chosse the appropriate generator based on your `task`.\n",
    "\n",
    "----\n",
    "\n",
    "### Train Your Arc-I Model\n",
    "\n",
    "To train a Arc-I model, we need to create an instance of ArcIModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchzoo import models\n",
    "arci_model = models.ArcIModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to set hyper-parameters to our Arc-I Model. In general, there are **two types of hyper-parameters**:\n",
    "\n",
    "**Required parameters**: For Arc-I, if you are using the pre-trained embeddings, you're required to set the embedding_mat and the size manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fitted parameters is stored in the `context` property of pre-processor instance during the training stage.\n",
    "from matchzoo import losses\n",
    "from matchzoo import tasks\n",
    "arci_model.params['task'] = tasks.Classification()\n",
    "arci_model.embedding_mat = processed_tr.context['embedding_mat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Tunable parameters**: For Arc-I, you're allowed to tune these parameters:\n",
    "\n",
    "```python\n",
    "from matchzoo import tasks\n",
    "\n",
    "        \n",
    "params = {\n",
    "            'input_shapes': [(32,), (32,)] # Lengths of matching texts.\n",
    "            'optimizer': 'adam' # By default, we use sgd. See keras optimizer.\n",
    "            'trainable_embedding': False # Whether finetune the embeddings.\n",
    "            'num_blocks': 1 # Number of the convolution layers.\n",
    "            'left_kernel_count': [32] # Number of filters in convolution. See keras Conv1D.\n",
    "            'left_kernel_size': [3] # The length of the 1D convolution window. See keras Conv1D.\n",
    "            'right_kernel_count': [32] # Number of filters in convolution. See keras Conv1D.\n",
    "            'right_kernel_size': [3] # The length of the 1D convolution window. See keras Conv1D.\n",
    "            'activation': 'relu' # Activation function in convolution. See keras Conv1D.\n",
    "            'left_pool_size': [2] # Size of the max pooling windows for the left text. See keras MaxPooling1D.\n",
    "            'right_pool_size': [2] # Size of the max pooling windows for rhe right text. See keras MaxPooling1D.\n",
    "            'padding': 'same' # Padding mode. See keras padding in Conv1D.   \n",
    "            'dropout_rate': 0.0 # Probability of an element to be zeroed. See keras Dropout.\n",
    "            'embedding_random_scale': 0.2 # The range of the random initialized embedding. \n",
    "            'task': tasks.Classification, # Default Classification, you can use tasks.Ranking\n",
    "            'loss': 'categorical_crossentropy', # categorical_crossentropy, see keras loss.\n",
    "            'metric': 'acc', # Accuracy by default, see keras metric.\n",
    "         }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arci parameters:  name                          ArcIModel\n",
      "model_class                   <class 'matchzoo.models.arci_model.ArcIModel'>\n",
      "input_shapes                  [(32,), (32,)]\n",
      "task                          <matchzoo.tasks.classification.Classification object at 0x7f73a4500320>\n",
      "metrics                       ['acc']\n",
      "loss                          categorical_crossentropy\n",
      "optimizer                     adam\n",
      "trainable_embedding           False\n",
      "embedding_dim                 300\n",
      "vocab_size                    3708\n",
      "num_blocks                    1\n",
      "left_kernel_count             [32]\n",
      "left_kernel_size              [3]\n",
      "right_kernel_count            [32]\n",
      "right_kernel_size             [3]\n",
      "activation                    relu\n",
      "left_pool_size                [2]\n",
      "right_pool_size               [2]\n",
      "padding                       same\n",
      "dropout_rate                  0.0\n",
      "embedding_mat                 [[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [-1.522      0.31011    0.17451   ...  0.21532    0.26868    0.65862  ]\n",
      " [-0.28341    0.040345  -0.24531   ...  0.17225   -0.42085   -0.0030652]\n",
      " ...\n",
      " [-0.22726    0.32929    0.46386   ...  0.032113   0.063718   0.54452  ]\n",
      " [ 0.18122   -0.13506   -0.0099825 ...  0.025173   0.18949   -0.83252  ]\n",
      " [-0.76288   -0.65198    0.050158  ...  0.21726   -1.0063    -0.37066  ]]\n",
      "embedding_random_scale        0.2\n"
     ]
    }
   ],
   "source": [
    "arci_model.guess_and_fill_missing_params()\n",
    "print('arci parameters: ', arci_model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "\n",
    "To train the model after all the parameters were settled, call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.2842 - acc: 0.9031\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 0.0218 - acc: 1.0000\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 8s 38ms/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 7.1924e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 5.3435e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 4.0054e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 3.1390e-04 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "arci_model.build()\n",
    "arci_model.compile()\n",
    "# Fit the arci model on generator.\n",
    "arci_model.fit_generator(generator_tr, steps_per_epoch=200, epochs=10)\n",
    "# Make predictions on the first batch of test data\n",
    "X_te, y_te = generator_te[0]\n",
    "predictions = arci_model.predict([X_te.text_left, X_te.text_right])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264/1265 is predicted as [0.17494294 0.825057  ]\n",
      "566/567 is predicted as [0.4681414 0.5318586]\n",
      "956/957 is predicted as [9.99893427e-01 1.06538624e-04]\n",
      "1188/1189 is predicted as [0.53175515 0.46824485]\n",
      "806/807 is predicted as [0.6687734  0.33122656]\n",
      "796/797 is predicted as [0.9432251  0.05677492]\n",
      "1886/1887 is predicted as [0.99815995 0.00184011]\n",
      "834/835 is predicted as [0.8919989  0.10800117]\n",
      "42/43 is predicted as [9.9982929e-01 1.7065427e-04]\n",
      "1566/1567 is predicted as [9.9985564e-01 1.4437299e-04]\n",
      "800/801 is predicted as [0.998869   0.00113099]\n",
      "1238/1239 is predicted as [9.9998116e-01 1.8888943e-05]\n",
      "1906/1907 is predicted as [9.9990618e-01 9.3757066e-05]\n",
      "1274/1275 is predicted as [9.9951077e-01 4.8926729e-04]\n",
      "1344/1345 is predicted as [0.97073054 0.02926948]\n",
      "596/597 is predicted as [0.50249165 0.4975083 ]\n",
      "1224/1225 is predicted as [0.157969   0.84203106]\n",
      "1340/1341 is predicted as [0.75901395 0.24098605]\n",
      "646/647 is predicted as [0.5194546 0.4805454]\n",
      "574/575 is predicted as [9.9930036e-01 6.9961377e-04]\n",
      "1392/1393 is predicted as [0.99481153 0.00518842]\n",
      "720/721 is predicted as [0.7408859  0.25911406]\n",
      "464/465 is predicted as [0.9972084  0.00279153]\n",
      "458/459 is predicted as [0.2944938  0.70550615]\n",
      "1706/1707 is predicted as [0.48117054 0.51882946]\n",
      "1016/1017 is predicted as [9.9995399e-01 4.6066598e-05]\n",
      "818/819 is predicted as [9.994653e-01 5.346898e-04]\n",
      "598/599 is predicted as [9.9995029e-01 4.9664446e-05]\n",
      "302/303 is predicted as [0.9746225  0.02537747]\n",
      "1754/1755 is predicted as [0.99760914 0.0023908 ]\n",
      "436/437 is predicted as [0.8726713  0.12732866]\n",
      "1394/1395 is predicted as [0.00977541 0.99022454]\n",
      "1554/1555 is predicted as [9.999654e-01 3.460596e-05]\n",
      "376/377 is predicted as [0.07392078 0.9260792 ]\n",
      "284/285 is predicted as [0.83541 0.16459]\n",
      "1546/1547 is predicted as [0.9854274  0.01457267]\n",
      "1776/1777 is predicted as [9.9959141e-01 4.0858262e-04]\n",
      "1024/1025 is predicted as [0.99853444 0.00146553]\n",
      "842/843 is predicted as [0.9950701  0.00492991]\n",
      "1846/1847 is predicted as [0.9983486  0.00165145]\n",
      "1126/1127 is predicted as [0.9697234  0.03027657]\n",
      "576/577 is predicted as [0.9914562  0.00854375]\n",
      "1848/1849 is predicted as [0.17135271 0.8286473 ]\n",
      "1290/1291 is predicted as [0.9640141  0.03598584]\n",
      "438/439 is predicted as [0.9772584  0.02274158]\n",
      "1680/1681 is predicted as [0.9596798  0.04032019]\n",
      "246/247 is predicted as [0.53860265 0.4613973 ]\n",
      "1991/1992 is predicted as [0.99276465 0.00723538]\n",
      "1234/1235 is predicted as [0.99884546 0.00115452]\n",
      "380/381 is predicted as [9.9998605e-01 1.3945406e-05]\n",
      "1612/1613 is predicted as [0.91609746 0.08390254]\n",
      "292/293 is predicted as [0.97926086 0.02073912]\n",
      "1156/1157 is predicted as [0.791533   0.20846704]\n",
      "586/587 is predicted as [0.9909334  0.00906656]\n",
      "336/337 is predicted as [9.9974245e-01 2.5752201e-04]\n",
      "378/379 is predicted as [0.62588644 0.3741135 ]\n",
      "1148/1149 is predicted as [0.2158974 0.7841026]\n",
      "1292/1293 is predicted as [9.9978071e-01 2.1933894e-04]\n",
      "1296/1297 is predicted as [0.05977486 0.9402252 ]\n",
      "668/669 is predicted as [0.01544964 0.9845503 ]\n",
      "746/747 is predicted as [9.9999976e-01 2.7707534e-07]\n",
      "1780/1781 is predicted as [0.81124324 0.18875672]\n",
      "1502/1503 is predicted as [0.97137713 0.02862292]\n",
      "782/783 is predicted as [0.9797165 0.0202835]\n"
     ]
    }
   ],
   "source": [
    "for id_left, id_right, pred in zip(X_te.id_left, X_te.id_right, predictions):\n",
    "    print(\"{}/{} is predicted as {}\".format(id_left, id_right, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Persistence\n",
    "\n",
    "You can persist your trained model using `model.save()` and `load_model` function:\n",
    "\n",
    "```python\n",
    "from matchzoo import engine\n",
    "# Save the model to dir.\n",
    "arci_model.save('/your-model-saved-path')\n",
    "# And load the model from dir.\n",
    "engine.load_model('/your-model-saved-path')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[Hu et al. 2014] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. \"Convolutional neural network architectures for matching natural language sentences.\" In Advances in neural information processing systems (NIPS), pp. 2042-2050. 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matchzoo",
   "language": "python",
   "name": "matchzoo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
